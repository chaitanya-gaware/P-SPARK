{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 16:54:34 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.5 instead (on interface wlp0s20f3)\n",
      "24/04/17 16:54:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/17 16:54:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName(\"group_window\")\\\n",
    "                    .getOrCreate()\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-----------+\n",
      "| ID|Product Name| Price|Total Value|\n",
      "+---+------------+------+-----------+\n",
      "|  1|      Jacket|872.19|    7849.71|\n",
      "|  2|       Jeans|865.39|    1730.78|\n",
      "|  3|         Hat|183.11|     549.33|\n",
      "|  4|     Sweater|305.15|      610.3|\n",
      "|  5|         Hat| 84.59|      845.9|\n",
      "|  6|       Socks|191.72|    1533.76|\n",
      "|  7|      Jacket|696.02|     696.02|\n",
      "|  8|     Sweater|170.31|    1532.79|\n",
      "|  9|       Jeans|163.04|    1141.28|\n",
      "| 10|     Sweater| 990.3|     1980.6|\n",
      "| 11|     Sweater| 92.88|     743.04|\n",
      "| 12|       Shoes|306.33|    1531.65|\n",
      "| 13|       Shirt| 65.67|     197.01|\n",
      "| 14|      Jacket|555.63|    4445.04|\n",
      "| 15|       Jeans| 48.99|     342.93|\n",
      "| 16|      Jacket|323.87|    2590.96|\n",
      "| 17|      Jacket| 895.2|     8952.0|\n",
      "| 18|       Shoes|106.37|     531.85|\n",
      "| 19|         Hat|146.77|     880.62|\n",
      "| 20|       Socks|584.94|    5264.46|\n",
      "+---+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",True)\\\n",
    "                .option(\"inferschema\",True)\\\n",
    "                .load(\"./sales_transactions.csv\")\n",
    "                \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=================================================>    (182 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| ID|total_value|\n",
      "+---+-----------+\n",
      "|  1|    7849.71|\n",
      "|  2|    1730.78|\n",
      "|  3|     549.33|\n",
      "|  4|      610.3|\n",
      "|  5|      845.9|\n",
      "|  6|    1533.76|\n",
      "|  7|     696.02|\n",
      "|  8|    1532.79|\n",
      "|  9|    1141.28|\n",
      "| 10|     1980.6|\n",
      "| 11|     743.04|\n",
      "| 12|    1531.65|\n",
      "| 13|     197.01|\n",
      "| 14|    4445.04|\n",
      "| 15|     342.93|\n",
      "| 16|    2590.96|\n",
      "| 17|     8952.0|\n",
      "| 18|     531.85|\n",
      "| 19|     880.62|\n",
      "| 20|    5264.46|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(\"ID\").agg(sum(col(\"Total Value\")).alias(\"total_value\")).orderBy(\"ID\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WindowSpec' object has no attribute 'desc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# window_specs = Window.orderBy(col(\"total_value\"),desc())\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m window_specs \u001b[38;5;241m=\u001b[39m \u001b[43mWindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WindowSpec' object has no attribute 'desc'"
     ]
    }
   ],
   "source": [
    "# window_specs = Window.orderBy(col(\"total_value\"),desc())\n",
    "window_specs = Window.orderBy(\"total_value\").desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 17:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 41:===================================================>  (192 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+\n",
      "| ID|total_value|rank|\n",
      "+---+-----------+----+\n",
      "|212|     9973.0|   1|\n",
      "| 94|     9757.9|   2|\n",
      "|148|     9648.3|   3|\n",
      "|204|     9554.5|   4|\n",
      "|471|     9547.8|   5|\n",
      "|160|     9502.0|   6|\n",
      "|626|     9435.7|   7|\n",
      "|570|     9357.0|   8|\n",
      "|355|     9286.9|   9|\n",
      "|266|     9164.3|  10|\n",
      "|856|     9077.4|  11|\n",
      "|311|     9028.7|  12|\n",
      "| 17|     8952.0|  13|\n",
      "|725|     8948.2|  14|\n",
      "|283|    8944.29|  15|\n",
      "| 86|     8895.3|  16|\n",
      "| 33|     8835.5|  17|\n",
      "|142|    8808.66|  18|\n",
      "|290|    8783.37|  19|\n",
      "|850|    8774.55|  20|\n",
      "+---+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_specs = Window.orderBy(\"total_value\").desc()\n",
    "df2 = df1.withColumn(\"rank\", dense_rank().over(window_specs))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 17:19:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 47:=================================================>    (182 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+\n",
      "| ID|total_value|rank|\n",
      "+---+-----------+----+\n",
      "|212|     9973.0|   1|\n",
      "| 94|     9757.9|   2|\n",
      "|148|     9648.3|   3|\n",
      "|204|     9554.5|   4|\n",
      "|471|     9547.8|   5|\n",
      "|160|     9502.0|   6|\n",
      "|626|     9435.7|   7|\n",
      "|570|     9357.0|   8|\n",
      "|355|     9286.9|   9|\n",
      "|266|     9164.3|  10|\n",
      "|856|     9077.4|  11|\n",
      "|311|     9028.7|  12|\n",
      "| 17|     8952.0|  13|\n",
      "|725|     8948.2|  14|\n",
      "|283|    8944.29|  15|\n",
      "| 86|     8895.3|  16|\n",
      "| 33|     8835.5|  17|\n",
      "|142|    8808.66|  18|\n",
      "|290|    8783.37|  19|\n",
      "|850|    8774.55|  20|\n",
      "+---+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_specs = Window.orderBy(desc(\"total_value\"))\n",
    "df2 = df1.withColumn(\"rank\", dense_rank().over(window_specs))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
