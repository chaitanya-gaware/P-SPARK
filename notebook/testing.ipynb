{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|2142|    1|\n",
      "|2122|    1|\n",
      "|9427|    1|\n",
      "|9465|    1|\n",
      "|1591|    1|\n",
      "|1342|    2|\n",
      "|2366|    1|\n",
      "|2866|    1|\n",
      "|1342|    2|\n",
      "|5803|    2|\n",
      "| 833|    1|\n",
      "|4935|    1|\n",
      "|9427|    2|\n",
      "|1645|    1|\n",
      "| 148|    2|\n",
      "| 148|    2|\n",
      "|2142|    2|\n",
      "|1580|    2|\n",
      "| 496|    1|\n",
      "|7880|    1|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate random data\n",
    "data = [(random.randint(0, 9999), random.randint(1, 2)) for _ in range(11000)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Repartition DataFrame\n",
    "df = df.repartition(\"key\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n",
    "\n",
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|3.536978925942362E-4|-7.27272775356920...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         97|   92|\n",
      "|         21|   90|\n",
      "|        117|   76|\n",
      "|         77|   75|\n",
      "|        162|   75|\n",
      "|        106|   75|\n",
      "|         35|   74|\n",
      "|        114|   74|\n",
      "|         69|   73|\n",
      "|        112|   72|\n",
      "|        103|   71|\n",
      "|        147|   71|\n",
      "|        122|   70|\n",
      "|        175|   69|\n",
      "|         25|   69|\n",
      "|        115|   69|\n",
      "|         18|   69|\n",
      "|          4|   69|\n",
      "|        150|   68|\n",
      "|        198|   68|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|1088|    2|\n",
      "|7982|    2|\n",
      "|3997|    2|\n",
      "|8389|    2|\n",
      "|6357|    1|\n",
      "|5518|    2|\n",
      "|9376|    2|\n",
      "|7833|    2|\n",
      "|7253|    1|\n",
      "|8638|    1|\n",
      "|4101|    2|\n",
      "|9852|    2|\n",
      "|8086|    1|\n",
      "|6336|    2|\n",
      "|2659|    2|\n",
      "|4818|    2|\n",
      "| 463|    1|\n",
      "|2122|    2|\n",
      "|5803|    1|\n",
      "|5803|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|-0.01051545026188...|-3.63636369646860...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:===========================================>          (161 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         97|   88|\n",
      "|        113|   86|\n",
      "|         21|   86|\n",
      "|        150|   84|\n",
      "|         13|   80|\n",
      "|        176|   77|\n",
      "|        117|   77|\n",
      "|        162|   76|\n",
      "|        100|   76|\n",
      "|        143|   74|\n",
      "|         86|   74|\n",
      "|        102|   74|\n",
      "|        190|   73|\n",
      "|        163|   72|\n",
      "|         23|   72|\n",
      "|        110|   72|\n",
      "|         51|   71|\n",
      "|         83|   71|\n",
      "|        121|   70|\n",
      "|        171|   70|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import skewness, count, spark_partition_id, col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate random data\n",
    "data = [(random.randint(0, 9999), random.randint(1, 2)) for _ in range(11000)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Repartition DataFrame\n",
    "df = df.repartition(\"key\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Compute skewness for each column\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n",
    "\n",
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         15|  751|\n",
      "|         10|  749|\n",
      "|         13|  749|\n",
      "|          9|  749|\n",
      "|          8|  749|\n",
      "|         11|  749|\n",
      "|         12|  749|\n",
      "|         14|  749|\n",
      "|          3|    1|\n",
      "|          5|    1|\n",
      "|          1|    1|\n",
      "|          6|    1|\n",
      "|          7|    1|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Skewness Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with skewed data\n",
    "data = [(\"A\",), (\"A\",), (\"A\",), (\"B\",), (\"B\",), (\"C\",)]\n",
    "columns = [\"col\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Replicate data in the skewed column to introduce skewness\n",
    "skewed_data = []\n",
    "for i in range(1, 1000):\n",
    "    skewed_data.extend(data)\n",
    "skewed_df = spark.createDataFrame(skewed_data, columns)\n",
    "\n",
    "# Union the original DataFrame with the skewed DataFrame\n",
    "df = df.union(skewed_df)\n",
    "\n",
    "# Add a column to the DataFrame with the partition ID\n",
    "df = df.withColumn(\"partitionId\", F.spark_partition_id())\n",
    "\n",
    "# Compute the number of records in each partition and sort the results\n",
    "skew_df = df.groupBy(\"partitionId\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Display the results\n",
    "skew_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|col|partitionId|\n",
      "+---+-----------+\n",
      "|  A|          0|\n",
      "|  A|          0|\n",
      "|  A|          0|\n",
      "|  B|          0|\n",
      "|  B|          0|\n",
      "|  C|          0|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  B|          1|\n",
      "|  B|          1|\n",
      "|  C|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  B|          1|\n",
      "|  B|          1|\n",
      "|  C|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 195:====================================>                (138 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          1| 5994|\n",
      "|          0|    6|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the number of records in each partition and sort the results\n",
    "df = df.groupBy(\"partitionId\").count().orderBy(\"partitionId\", ascending=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|part|count|\n",
      "+----+-----+\n",
      "|   0|  599|\n",
      "|   1|  600|\n",
      "|   2|  600|\n",
      "|   3|  600|\n",
      "|   4|  600|\n",
      "|   5|  600|\n",
      "|   6|  601|\n",
      "|   7|  600|\n",
      "|   8|  600|\n",
      "|   9|  600|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_df = df1.repartition(10)\n",
    "re_df = re_df.withColumn(\"part\", spark_partition_id())\n",
    "# re_df = re_df.groupBy(\"partitionId\").count().orderBy(\"partitionId\", ascending=False)\n",
    "\n",
    "\n",
    "re_df = re_df.groupBy(\"part\").count().orderBy(\"part\",ascending=True)\n",
    "re_df.show()\n",
    "re_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def max_profit(prices):\n",
    "    if not prices:\n",
    "        return 0\n",
    "    \n",
    "    min_price = prices[0]\n",
    "    max_profit = 0\n",
    "    \n",
    "    for price in prices:\n",
    "        min_price = min(min_price, price)\n",
    "        max_profit = max(max_profit, price - min_price)\n",
    "    \n",
    "    return max_profit\n",
    "\n",
    "# Example usage:\n",
    "prices = [7, 1, 5, 3, 6, 4]\n",
    "print(max_profit(prices))  # Output: 5 (Buy at price 1 and sell at price 6 for a profit of 6 - 1 = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=========================================>            (154 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          1| 5994|\n",
      "|          0|    6|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
