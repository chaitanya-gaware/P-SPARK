{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|2142|    1|\n",
      "|2122|    1|\n",
      "|9427|    1|\n",
      "|9465|    1|\n",
      "|1591|    1|\n",
      "|1342|    2|\n",
      "|2366|    1|\n",
      "|2866|    1|\n",
      "|1342|    2|\n",
      "|5803|    2|\n",
      "| 833|    1|\n",
      "|4935|    1|\n",
      "|9427|    2|\n",
      "|1645|    1|\n",
      "| 148|    2|\n",
      "| 148|    2|\n",
      "|2142|    2|\n",
      "|1580|    2|\n",
      "| 496|    1|\n",
      "|7880|    1|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate random data\n",
    "data = [(random.randint(0, 9999), random.randint(1, 2)) for _ in range(11000)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Repartition DataFrame\n",
    "df = df.repartition(\"key\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n",
    "\n",
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|3.536978925942362E-4|-7.27272775356920...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         97|   92|\n",
      "|         21|   90|\n",
      "|        117|   76|\n",
      "|         77|   75|\n",
      "|        162|   75|\n",
      "|        106|   75|\n",
      "|         35|   74|\n",
      "|        114|   74|\n",
      "|         69|   73|\n",
      "|        112|   72|\n",
      "|        103|   71|\n",
      "|        147|   71|\n",
      "|        122|   70|\n",
      "|        175|   69|\n",
      "|         25|   69|\n",
      "|        115|   69|\n",
      "|         18|   69|\n",
      "|          4|   69|\n",
      "|        150|   68|\n",
      "|        198|   68|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(F.col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| key|value|\n",
      "+----+-----+\n",
      "|1088|    2|\n",
      "|7982|    2|\n",
      "|3997|    2|\n",
      "|8389|    2|\n",
      "|6357|    1|\n",
      "|5518|    2|\n",
      "|9376|    2|\n",
      "|7833|    2|\n",
      "|7253|    1|\n",
      "|8638|    1|\n",
      "|4101|    2|\n",
      "|9852|    2|\n",
      "|8086|    1|\n",
      "|6336|    2|\n",
      "|2659|    2|\n",
      "|4818|    2|\n",
      "| 463|    1|\n",
      "|2122|    2|\n",
      "|5803|    1|\n",
      "|5803|    2|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|                 key|               value|\n",
      "+--------------------+--------------------+\n",
      "|-0.01051545026188...|-3.63636369646860...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:===========================================>          (161 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         97|   88|\n",
      "|        113|   86|\n",
      "|         21|   86|\n",
      "|        150|   84|\n",
      "|         13|   80|\n",
      "|        176|   77|\n",
      "|        117|   77|\n",
      "|        162|   76|\n",
      "|        100|   76|\n",
      "|        143|   74|\n",
      "|         86|   74|\n",
      "|        102|   74|\n",
      "|        190|   73|\n",
      "|        163|   72|\n",
      "|         23|   72|\n",
      "|        110|   72|\n",
      "|         51|   71|\n",
      "|         83|   71|\n",
      "|        121|   70|\n",
      "|        171|   70|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import skewness, count, spark_partition_id, col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate random data\n",
    "data = [(random.randint(0, 9999), random.randint(1, 2)) for _ in range(11000)]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"key\", IntegerType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Repartition DataFrame\n",
    "df = df.repartition(\"key\")\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Compute skewness for each column\n",
    "skew_result = df.select([skewness(col).alias(col) for col in df.columns])\n",
    "skew_result.show()\n",
    "\n",
    "# Add partitionId column\n",
    "df = df.withColumn(\"partitionId\", spark_partition_id())\n",
    "\n",
    "# Calculate count of keys per partition\n",
    "skew_df = df.select(\"partitionId\", \"key\").groupBy(\"partitionId\").agg(count(\"key\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
    "\n",
    "# Display the skewness DataFrame\n",
    "skew_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|         15|  751|\n",
      "|         10|  749|\n",
      "|         13|  749|\n",
      "|          9|  749|\n",
      "|          8|  749|\n",
      "|         11|  749|\n",
      "|         12|  749|\n",
      "|         14|  749|\n",
      "|          3|    1|\n",
      "|          5|    1|\n",
      "|          1|    1|\n",
      "|          6|    1|\n",
      "|          7|    1|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Skewness Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with skewed data\n",
    "data = [(\"A\",), (\"A\",), (\"A\",), (\"B\",), (\"B\",), (\"C\",)]\n",
    "columns = [\"col\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Replicate data in the skewed column to introduce skewness\n",
    "skewed_data = []\n",
    "for i in range(1, 1000):\n",
    "    skewed_data.extend(data)\n",
    "skewed_df = spark.createDataFrame(skewed_data, columns)\n",
    "\n",
    "# Union the original DataFrame with the skewed DataFrame\n",
    "df = df.union(skewed_df)\n",
    "\n",
    "# Add a column to the DataFrame with the partition ID\n",
    "df = df.withColumn(\"partitionId\", F.spark_partition_id())\n",
    "\n",
    "# Compute the number of records in each partition and sort the results\n",
    "skew_df = df.groupBy(\"partitionId\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Display the results\n",
    "skew_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "|  A|\n",
      "|  B|\n",
      "|  B|\n",
      "|  C|\n",
      "|  A|\n",
      "|  A|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|col|partitionId|\n",
      "+---+-----------+\n",
      "|  A|          0|\n",
      "|  A|          0|\n",
      "|  A|          0|\n",
      "|  B|          0|\n",
      "|  B|          0|\n",
      "|  C|          0|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  B|          1|\n",
      "|  B|          1|\n",
      "|  C|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "|  B|          1|\n",
      "|  B|          1|\n",
      "|  C|          1|\n",
      "|  A|          1|\n",
      "|  A|          1|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 195:====================================>                (138 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          1| 5994|\n",
      "|          0|    6|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute the number of records in each partition and sort the results\n",
    "df = df.groupBy(\"partitionId\").count().orderBy(\"partitionId\", ascending=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|part|count|\n",
      "+----+-----+\n",
      "|   0|  599|\n",
      "|   1|  600|\n",
      "|   2|  600|\n",
      "|   3|  600|\n",
      "|   4|  600|\n",
      "|   5|  600|\n",
      "|   6|  601|\n",
      "|   7|  600|\n",
      "|   8|  600|\n",
      "|   9|  600|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_df = df1.repartition(10)\n",
    "re_df = re_df.withColumn(\"part\", spark_partition_id())\n",
    "# re_df = re_df.groupBy(\"partitionId\").count().orderBy(\"partitionId\", ascending=False)\n",
    "\n",
    "\n",
    "re_df = re_df.groupBy(\"part\").count().orderBy(\"part\",ascending=True)\n",
    "re_df.show()\n",
    "re_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def max_profit(prices):\n",
    "    if not prices:\n",
    "        return 0\n",
    "    \n",
    "    min_price = prices[0]\n",
    "    max_profit = 0\n",
    "    \n",
    "    for price in prices:\n",
    "        min_price = min(min_price, price)\n",
    "        max_profit = max(max_profit, price - min_price)\n",
    "    \n",
    "    return max_profit\n",
    "\n",
    "# Example usage:\n",
    "prices = [7, 1, 5, 3, 6, 4]\n",
    "print(max_profit(prices))  # Output: 5 (Buy at price 1 and sell at price 6 for a profit of 6 - 1 = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/chaitanya/GIT/P-SPARK/notebook/11-sc2/data.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscd2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpermissive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m11-sc2/data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     10\u001b[0m update_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermissive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/chaitanya/GIT/P-SPARK/11-sc2/data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/pyspark/sql/readwriter.py:204\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/chaitanya/GIT/P-SPARK/notebook/11-sc2/data.csv"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=========================================>            (154 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside function: 20\n",
      "Outside function: 10\n"
     ]
    }
   ],
   "source": [
    "x = 10  # Global variable\n",
    "\n",
    "def my_function():\n",
    "    x = 20  # This creates a new local variable 'x'\n",
    "    print(\"Inside function:\", x)\n",
    "\n",
    "my_function()\n",
    "print(\"Outside function:\", x)  # Output: Outside function: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside function: 20\n",
      "Outside function: 20\n"
     ]
    }
   ],
   "source": [
    "x = 10  # Global variable\n",
    "\n",
    "def my_function():\n",
    "    global x  # Declare 'x' as a global variable\n",
    "    x = 20  # Modifying the global variable 'x'\n",
    "    print(\"Inside function:\", x)\n",
    "\n",
    "my_function()\n",
    "print(\"Outside function:\", x)  # Output: Outside function: 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+\n",
      "| id|   name|      date|   sell|\n",
      "+---+-------+----------+-------+\n",
      "|  1| iphone|01-01-2023|1500000|\n",
      "|  2|samsung|01-01-2023|1100000|\n",
      "|  3|oneplus|01-01-2023|1100000|\n",
      "|  1| iphone|01-02-2023|1300000|\n",
      "|  2|samsung|01-02-2023|1120000|\n",
      "|  3|oneplus|01-02-2023|1120000|\n",
      "|  1| iphone|01-03-2023|1600000|\n",
      "|  2|samsung|01-03-2023|1080000|\n",
      "|  3|oneplus|01-03-2023|1160000|\n",
      "|  1| iphone|01-04-2023|1700000|\n",
      "|  2|samsung|01-04-2023|1800000|\n",
      "|  3|oneplus|01-04-2023|1170000|\n",
      "|  1| iphone|01-05-2023|1200000|\n",
      "|  2|samsung|01-05-2023| 980000|\n",
      "|  3|oneplus|01-05-2023|1175000|\n",
      "|  1| iphone|01-06-2023|1100000|\n",
      "|  2|samsung|01-06-2023|1100000|\n",
      "|  3|oneplus|01-06-2023|1200000|\n",
      "+---+-------+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+-----------+\n",
      "| id|   name|      date|   sell|previous_ms|\n",
      "+---+-------+----------+-------+-----------+\n",
      "|  1| iphone|01-01-2023|1500000|       null|\n",
      "|  1| iphone|01-02-2023|1300000|    1500000|\n",
      "|  1| iphone|01-03-2023|1600000|    1300000|\n",
      "|  1| iphone|01-04-2023|1700000|    1600000|\n",
      "|  1| iphone|01-05-2023|1200000|    1700000|\n",
      "|  1| iphone|01-06-2023|1100000|    1200000|\n",
      "|  2|samsung|01-01-2023|1100000|       null|\n",
      "|  2|samsung|01-02-2023|1120000|    1100000|\n",
      "|  2|samsung|01-03-2023|1080000|    1120000|\n",
      "|  2|samsung|01-04-2023|1800000|    1080000|\n",
      "|  2|samsung|01-05-2023| 980000|    1800000|\n",
      "|  2|samsung|01-06-2023|1100000|     980000|\n",
      "|  3|oneplus|01-01-2023|1100000|       null|\n",
      "|  3|oneplus|01-02-2023|1120000|    1100000|\n",
      "|  3|oneplus|01-03-2023|1160000|    1120000|\n",
      "|  3|oneplus|01-04-2023|1170000|    1160000|\n",
      "|  3|oneplus|01-05-2023|1175000|    1170000|\n",
      "|  3|oneplus|01-06-2023|1200000|    1175000|\n",
      "+---+-------+----------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+-----------+-------------------+\n",
      "| id|   name|      date|   sell|previous_ms|       loss_or_gain|\n",
      "+---+-------+----------+-------+-----------+-------------------+\n",
      "|  1| iphone|01-01-2023|1500000|       null|               null|\n",
      "|  1| iphone|01-02-2023|1300000|    1500000|-15.384615384615385|\n",
      "|  1| iphone|01-03-2023|1600000|    1300000|              18.75|\n",
      "|  1| iphone|01-04-2023|1700000|    1600000|  5.882352941176471|\n",
      "|  1| iphone|01-05-2023|1200000|    1700000|-41.666666666666664|\n",
      "|  1| iphone|01-06-2023|1100000|    1200000| -9.090909090909092|\n",
      "|  2|samsung|01-01-2023|1100000|       null|               null|\n",
      "|  2|samsung|01-02-2023|1120000|    1100000| 1.7857142857142858|\n",
      "|  2|samsung|01-03-2023|1080000|    1120000|-3.7037037037037037|\n",
      "|  2|samsung|01-04-2023|1800000|    1080000|               40.0|\n",
      "|  2|samsung|01-05-2023| 980000|    1800000|  -83.6734693877551|\n",
      "|  2|samsung|01-06-2023|1100000|     980000| 10.909090909090908|\n",
      "|  3|oneplus|01-01-2023|1100000|       null|               null|\n",
      "|  3|oneplus|01-02-2023|1120000|    1100000| 1.7857142857142858|\n",
      "|  3|oneplus|01-03-2023|1160000|    1120000| 3.4482758620689653|\n",
      "|  3|oneplus|01-04-2023|1170000|    1160000| 0.8547008547008547|\n",
      "|  3|oneplus|01-05-2023|1175000|    1170000|  0.425531914893617|\n",
      "|  3|oneplus|01-06-2023|1200000|    1175000| 2.0833333333333335|\n",
      "+---+-------+----------+-------+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-------+-----------+------------+\n",
      "| id|   name|      date|   sell|previous_ms|loss_or_gain|\n",
      "+---+-------+----------+-------+-----------+------------+\n",
      "|  1| iphone|01-01-2023|1500000|       null|        null|\n",
      "|  1| iphone|01-02-2023|1300000|    1500000|      -15.38|\n",
      "|  1| iphone|01-03-2023|1600000|    1300000|       18.75|\n",
      "|  1| iphone|01-04-2023|1700000|    1600000|        5.88|\n",
      "|  1| iphone|01-05-2023|1200000|    1700000|      -41.67|\n",
      "|  1| iphone|01-06-2023|1100000|    1200000|       -9.09|\n",
      "|  2|samsung|01-01-2023|1100000|       null|        null|\n",
      "|  2|samsung|01-02-2023|1120000|    1100000|        1.79|\n",
      "|  2|samsung|01-03-2023|1080000|    1120000|        -3.7|\n",
      "|  2|samsung|01-04-2023|1800000|    1080000|        40.0|\n",
      "|  2|samsung|01-05-2023| 980000|    1800000|      -83.67|\n",
      "|  2|samsung|01-06-2023|1100000|     980000|       10.91|\n",
      "|  3|oneplus|01-01-2023|1100000|       null|        null|\n",
      "|  3|oneplus|01-02-2023|1120000|    1100000|        1.79|\n",
      "|  3|oneplus|01-03-2023|1160000|    1120000|        3.45|\n",
      "|  3|oneplus|01-04-2023|1170000|    1160000|        0.85|\n",
      "|  3|oneplus|01-05-2023|1175000|    1170000|        0.43|\n",
      "|  3|oneplus|01-06-2023|1200000|    1175000|        2.08|\n",
      "+---+-------+----------+-------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "  \n",
    "  from pyspark.sql import SparkSession\n",
    "  spark = SparkSession.builder\\\n",
    "                      .master(\"local\") \\\n",
    "                      .appName(\"lead_lag\") \\\n",
    "                      .getOrCreate()\n",
    "  product_data = [\n",
    "  (1,\"iphone\",\"01-01-2023\",1500000),\n",
    "  (2,\"samsung\",\"01-01-2023\",1100000),\n",
    "  (3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "  (1,\"iphone\",\"01-02-2023\",1300000),\n",
    "  (2,\"samsung\",\"01-02-2023\",1120000),\n",
    "  (3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "  (1,\"iphone\",\"01-03-2023\",1600000),\n",
    "  (2,\"samsung\",\"01-03-2023\",1080000),\n",
    "  (3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "  (1,\"iphone\",\"01-04-2023\",1700000),\n",
    "  (2,\"samsung\",\"01-04-2023\",1800000),\n",
    "  (3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "  (1,\"iphone\",\"01-05-2023\",1200000),\n",
    "  (2,\"samsung\",\"01-05-2023\",980000),\n",
    "  (3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "  (1,\"iphone\",\"01-06-2023\",1100000),\n",
    "  (2,\"samsung\",\"01-06-2023\",1100000),\n",
    "  (3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "  ]\n",
    "  \n",
    "  \n",
    "  product_schema=['id','name','date','sell']\n",
    "  \n",
    "  p_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "  p_df.show()\n",
    "  \n",
    "  from pyspark.sql.functions import *\n",
    "  from pyspark.sql.types import *\n",
    "  from pyspark.sql.window import Window\n",
    "  \n",
    "  window = Window.partitionBy(\"name\").orderBy(\"date\")\n",
    "  \n",
    "  df = p_df.withColumn(\"previous_ms\", lag(p_df[\"sell\"],1).over(window))\n",
    "  df.show()\n",
    "  \n",
    "  df.withColumn(\"loss_or_gain\",((col(\"sell\") - col(\"previous_ms\"))*100/df[\"sell\"])).show()\n",
    "  \n",
    "  df.withColumn(\"loss_or_gain\",round(((col(\"sell\") - col(\"previous_ms\"))*100/df[\"sell\"]),2)).show() # showing round data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd efgh ijkl "
     ]
    }
   ],
   "source": [
    "l=[\"ab\", \"cd\", \"ef\", \"gh\", \"ij\", \"kl\"]\n",
    "\n",
    "m=2\n",
    "\n",
    "for i in range(0,6,m):\n",
    "\n",
    "    print(\"\".join(l[i:i+m]), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "class One:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.add(5)\n",
    "\n",
    "        print(self.i)\n",
    "\n",
    "    def add(self,i):\n",
    "\n",
    "        self.i=4+i;\n",
    "\n",
    "class Two (One):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def add(self,i):\n",
    "\n",
    "        self.i=2+i;\n",
    "\n",
    "work=Two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n"
     ]
    }
   ],
   "source": [
    "def fun():\n",
    "\n",
    "    a = '5'\n",
    "\n",
    "    b = '2.5'\n",
    "\n",
    "    def inc():\n",
    "\n",
    "        nonlocal a\n",
    "\n",
    "        nonlocal b\n",
    "\n",
    "        a = int(a) + float(b)\n",
    "\n",
    "        return a\n",
    "\n",
    "    return inc\n",
    "\n",
    "c = fun()\n",
    "\n",
    "print(c())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|part|  count|\n",
      "+----+-------+\n",
      "|   1|1793529|\n",
      "|   3|1780898|\n",
      "|   4| 513060|\n",
      "|   2|1842208|\n",
      "|   0|1884789|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.window  import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "path = \"/home/chaitanya/Downloads/cord19_medline_assocs_v2.3.tsv\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"tsv\")\\\n",
    "                    .master(\"local\")\\\n",
    "                    .getOrCreate()\n",
    "                    \n",
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",True)\\\n",
    "                .option(\"mode\",\"permissive\")\\\n",
    "                .option(\"delimiter\", \"\\t\")\\\n",
    "                .load(path)\n",
    "# df.show()\n",
    "# df.count()\n",
    "\n",
    "null_df = df.filter(col(\"src_ent\").isNull() | col(\"src_type\").isNull() | col(\"target_ent\").isNull() | col(\"score\").isNull() | col(\"debug\").isNull())\n",
    "# null_df.show()\n",
    "# print(null_df.count())\n",
    "s = df.rdd.getNumPartitions()\n",
    "print(s, type(s))\n",
    "skew_df = df.withColumn(\"part\", spark_partition_id())\n",
    "skew_calculate = skew_df.groupBy(\"part\").count()\n",
    "skew_calculate.show()\n",
    "# skew_df.show()\n",
    "# print(input(\"enter the value\"))\n",
    "# spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|part|  count|\n",
      "+----+-------+\n",
      "|   1|3635737|\n",
      "|   2|2293958|\n",
      "|   0|1884789|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.coalesce(3)\n",
    "skew_df = df1.withColumn(\"part\", spark_partition_id())\n",
    "skew_calculate = skew_df.groupBy(\"part\").count()\n",
    "skew_calculate.show()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
