{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/04 04:28:17 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.19 instead (on interface wlp0s20f3)\n",
      "23/12/04 04:28:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/04 04:28:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"withColumnRenamed\")\\\n",
    "                    .master(\"local\")\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"inferSchema\":True, \"escape\":\"\\\"\",\"header\":True} #\"mode\":\"permissive\",\"header\":True\n",
    "df = spark.read.format(\"csv\")\\\n",
    "                .options(**opt)\\\n",
    "                .load(\"currency.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Symbol\", \"Sym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumnRenamed in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumnRenamed(existing, new) method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by renaming an existing column.\n",
      "    This is a no-op if schema doesn't contain the given column name.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    existing : str\n",
      "        string, name of the existing column to rename.\n",
      "    new : str\n",
      "        string, new name of the column.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df.withColumnRenamed('age', 'age2').collect()\n",
      "    [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumnRenamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"a\", [1,2]), (\"b\", [2,3]), (\"c\", [3,4])]\n",
    "schema = (\"id\", \"number\")\n",
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|  a|[1, 2]|\n",
      "|  b|[2, 3]|\n",
      "|  c|[3, 4]|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- number: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "data = [(1,2,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+-----+\n",
      "| _c0|   _c1|                 _c2|  _c3|\n",
      "+----+------+--------------------+-----+\n",
      "|Code|Symbol|                Name|value|\n",
      "| AED|   د.إ|United Arab Emira...|   10|\n",
      "| AFN|     ؋|      Afghan afghani|   20|\n",
      "| ALL|     L|       Albanian, lek|   30|\n",
      "| AMD|   AMD|       Armenian dram|   40|\n",
      "| ANG|     ƒ|Netherlands Antil...|   50|\n",
      "| AOA|    Kz|      Angolan kwanza| null|\n",
      "| ARS|     $|      Argentine peso| null|\n",
      "| AUD|     $|   Australian dollar| null|\n",
      "| AWG|  Afl.|       Aruban florin| null|\n",
      "| AZN|   AZN|   Azerbaijani manat| null|\n",
      "| BAM|    KM|Bosnia and Herzeg...| null|\n",
      "| BBD|     $|    Barbadian dollar| null|\n",
      "| BDT|    ৳ |    Bangladeshi taka| null|\n",
      "| BGN|   лв.|       Bulgarian lev| null|\n",
      "| BHD|  .د.ب|      Bahraini dinar| null|\n",
      "| BIF|    Fr|     Burundian franc| null|\n",
      "| BMD|     $|    Bermudian dollar| null|\n",
      "| BND|     $|       Brunei dollar| null|\n",
      "| BOB|   Bs.|  Bolivian boliviano| null|\n",
      "+----+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\")\\\n",
    "                 .mode(\"overwrite\")\\\n",
    "                 .partitionBy(\"Symbol\")\\\n",
    "                 .saveAsTable(\"currency_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/chaitanya/GIT/P-SPARK/6-withColumnRenamed/spark-warehouse')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameWriter in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameWriter(OptionUtils)\n",
      " |  DataFrameWriter(df)\n",
      " |  \n",
      " |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameWriter\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bucketBy(self, numBuckets, col, *cols)\n",
      " |      Buckets the output by the given columns.If specified,\n",
      " |      the output is laid out on the file system similar to Hive's bucketing scheme.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numBuckets : int\n",
      " |          the number of buckets to save\n",
      " |      col : str, list or tuple\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Applicable for file-based data sources in combination with\n",
      " |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('bucketed_table'))\n",
      " |  \n",
      " |  csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)\n",
      " |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      " |              exists.\n",
      " |      \n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      sep : str, optional\n",
      " |          sets a separator (one or more characters) for each field and value. If None is\n",
      " |          set, it uses the default value, ``,``.\n",
      " |      quote : str, optional\n",
      " |          sets a single character used for escaping quoted values where the\n",
      " |          separator can be part of the value. If None is set, it uses the default\n",
      " |          value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
      " |      escape : str, optional\n",
      " |          sets a single character used for escaping quotes inside an already\n",
      " |          quoted value. If None is set, it uses the default value, ``\\``\n",
      " |      escapeQuotes : str or bool, optional\n",
      " |          a flag indicating whether values containing quotes should always\n",
      " |          be enclosed in quotes. If None is set, it uses the default value\n",
      " |          ``true``, escaping all values containing a quote character.\n",
      " |      quoteAll : str or bool, optional\n",
      " |          a flag indicating whether all values should always be enclosed in\n",
      " |          quotes. If None is set, it uses the default value ``false``,\n",
      " |          only escaping values containing a quote character.\n",
      " |      header : str or bool, optional\n",
      " |          writes the names of columns as the first line. If None is set, it uses\n",
      " |          the default value, ``false``.\n",
      " |      nullValue : str, optional\n",
      " |          sets the string representation of a null value. If None is set, it uses\n",
      " |          the default value, empty string.\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats follow\n",
      " |          the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      ignoreLeadingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not leading whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      ignoreTrailingWhiteSpace : str or bool, optional\n",
      " |          a flag indicating whether or not trailing whitespaces from\n",
      " |          values being written should be skipped. If None is set, it\n",
      " |          uses the default value, ``true``.\n",
      " |      charToEscapeQuoteEscaping : str, optional\n",
      " |          sets a single character used for escaping the escape for\n",
      " |          the quote character. If None is set, the default value is\n",
      " |          escape character when escape and quote characters are\n",
      " |          different, ``\\0`` otherwise..\n",
      " |      encoding : str, optional\n",
      " |          sets the encoding (charset) of saved csv files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      emptyValue : str, optional\n",
      " |          sets the string representation of an empty value. If None is set, it uses\n",
      " |          the default value, ``\"\"``.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\\\n``. Maximum length is 1 character.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the underlying output data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  insertInto(self, tableName, overwrite=None)\n",
      " |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      " |      \n",
      " |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      " |      schema of the table.\n",
      " |      \n",
      " |      Optionally overwriting any existing data.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, mode=None, properties=None)\n",
      " |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      url : str\n",
      " |          a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      table : str\n",
      " |          Name of the table in the external database.\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      properties : dict\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |  \n",
      " |  json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None, ignoreNullFields=None)\n",
      " |      Saves the content of the :class:`DataFrame` in JSON format\n",
      " |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      " |      specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats\n",
      " |          follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      encoding : str, optional\n",
      " |          specifies encoding (charset) of saved json files. If None is set,\n",
      " |          the default UTF-8 charset will be used.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      ignoreNullFields : str or bool, optional\n",
      " |          Whether to ignore null fields when generating JSON objects.\n",
      " |          If None is set, it uses the default value, ``true``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  mode(self, saveMode)\n",
      " |      Specifies the behavior when data or table already exists.\n",
      " |      \n",
      " |      Options include:\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an output option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds output options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for writing files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to format\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, snappy, zlib, and lzo).\n",
      " |          This will override ``orc.compress`` and\n",
      " |          ``spark.sql.orc.compression.codec``. If None is set, it uses the value\n",
      " |          specified in ``spark.sql.orc.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  parquet(self, path, mode=None, partitionBy=None, compression=None)\n",
      " |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, uncompressed, snappy, gzip,\n",
      " |          lzo, brotli, lz4, and zstd). This will override\n",
      " |          ``spark.sql.parquet.compression.codec``. If None is set, it uses the\n",
      " |          value specified in ``spark.sql.parquet.compression.codec``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  partitionBy(self, *cols)\n",
      " |      Partitions the output by the given columns on the file system.\n",
      " |      \n",
      " |      If specified, the output is laid out on the file system similar\n",
      " |      to Hive's partitioning scheme.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str or list\n",
      " |          name of columns\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  save(self, path=None, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      " |      \n",
      " |      The data source is specified by the ``format`` and a set of ``options``.\n",
      " |      If ``format`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, optional\n",
      " |          the path in a Hadoop supported file system\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : list, optional\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.write.mode(\"append\").save(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      " |  \n",
      " |  saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options)\n",
      " |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      " |      \n",
      " |      In the case the table already exists, behavior of this function depends on the\n",
      " |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      " |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      " |      the same as that of the existing table.\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          the table name\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      " |      partitionBy : str or list\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |  \n",
      " |  sortBy(self, col, *cols)\n",
      " |      Sorts the output in each bucket by the given columns on the file system.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : str, tuple or list\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> (df.write.format('parquet')  # doctest: +SKIP\n",
      " |      ...     .bucketBy(100, 'year', 'month')\n",
      " |      ...     .sortBy('day')\n",
      " |      ...     .mode(\"overwrite\")\n",
      " |      ...     .saveAsTable('sorted_bucketed_table'))\n",
      " |  \n",
      " |  text(self, path, compression=None, lineSep=None)\n",
      " |      Saves the content of the DataFrame in a text file at the specified path.\n",
      " |      The text files will be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      compression : str, optional\n",
      " |          compression codec to use when saving to file. This can be one of the\n",
      " |          known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      " |          snappy and deflate).\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for writing. If None is\n",
      " |          set, it uses the default value, ``\\n``.\n",
      " |      \n",
      " |      The DataFrame must have only one column that is of string type.\n",
      " |      Each row becomes a new line in the output file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listDatabases()\n",
    "help(df.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
