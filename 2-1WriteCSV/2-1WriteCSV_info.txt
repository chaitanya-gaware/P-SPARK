- here it is explain how can you write the file in the csv format using spark 
- file is written in CSV after performing some operation in real life scenario 
- Help on DataFrameWriter in module pyspark.sql.readwriter object:

class DataFrameWriter(OptionUtils)
 |  DataFrameWriter(df: 'DataFrame')
 |  
 |  Interface used to write a :class:`DataFrame` to external storage systems
 |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`
 |  to access this.
 |  
 |  .. versionadded:: 1.4.0
 |  
 |  .. versionchanged:: 3.4.0
 |      Supports Spark Connect.
 |  
 |  Method resolution order:
 |      DataFrameWriter
 |      OptionUtils
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, df: 'DataFrame')
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'
 |      Buckets the output by the given columns. If specified,
 |      the output is laid out on the file system similar to Hive's bucketing scheme,
 |      but with a different bucket hash function and is not compatible with Hive's bucketing.
 |      
 |      .. versionadded:: 2.3.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      numBuckets : int
 |          the number of buckets to save
 |      col : str, list or tuple
 |          a name of a column, or a list of names.
 |      cols : str
 |          additional names (optional). If `col` is a list it should be empty.
 |      
 |      Notes
 |      -----
 |      Applicable for file-based data sources in combination with
 |      :py:meth:`DataFrameWriter.saveAsTable`.
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.
 |      
 |      >>> from pyspark.sql.functions import input_file_name
 |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.
 |      ... _ = spark.sql("DROP TABLE IF EXISTS bucketed_table")
 |      >>> spark.createDataFrame([
 |      ...     (100, "Hyukjin Kwon"), (120, "Hyukjin Kwon"), (140, "Haejoon Lee")],
 |      ...     schema=["age", "name"]
 |      ... ).write.bucketBy(2, "name").mode("overwrite").saveAsTable("bucketed_table")
 |      >>> # Read the Parquet file as a DataFrame.
 |      ... spark.read.table("bucketed_table").sort("age").show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      |120|Hyukjin Kwon|
 |      |140| Haejoon Lee|
 |      +---+------------+
 |      >>> _ = spark.sql("DROP TABLE bucketed_table")
 |  
 |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None
 |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.
 |      
 |      .. versionadded:: 2.0.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str
 |          the path in any Hadoop supported file system
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \
 |              exists.
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a CSV file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a CSV file
 |      ...     df = spark.createDataFrame([{"age": 100, "name": "Hyukjin Kwon"}])
 |      ...     df.write.csv(d, mode="overwrite")
 |      ...
 |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.
 |      ...     spark.read.schema(df.schema).format("csv").option(
 |      ...         "nullValue", "Hyukjin Kwon").load(d).show()
 |      +---+----+
 |      |age|name|
 |      +---+----+
 |      |100|NULL|
 |      +---+----+
 |  
 |  format(self, source: str) -> 'DataFrameWriter'
 |      Specifies the underlying output data source.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      source : str
 |          string, name of the data source, e.g. 'json', 'parquet'.
 |      
 |      Examples
 |      --------
 |      >>> spark.range(1).write.format('parquet')
 |      <...readwriter.DataFrameWriter object ...>
 |      
 |      Write a DataFrame into a Parquet file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a Parquet file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.mode("overwrite").format("parquet").save(d)
 |      ...
 |      ...     # Read the Parquet file as a DataFrame.
 |      ...     spark.read.format('parquet').load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None
 |      Inserts the content of the :class:`DataFrame` to the specified table.
 |      
 |      It requires that the schema of the :class:`DataFrame` is the same as the
 |      schema of the table.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      overwrite : bool, optional
 |          If true, overwrites existing data. Disabled by default
 |      
 |      Notes
 |      -----
 |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores
 |      the column names and just uses position-based resolution.
 |      
 |      Examples
 |      --------
 |      >>> _ = spark.sql("DROP TABLE IF EXISTS tblA")
 |      >>> df = spark.createDataFrame([
 |      ...     (100, "Hyukjin Kwon"), (120, "Hyukjin Kwon"), (140, "Haejoon Lee")],
 |      ...     schema=["age", "name"]
 |      ... )
 |      >>> df.write.saveAsTable("tblA")
 |      
 |      Insert the data into 'tblA' table but with different column names.
 |      
 |      >>> df.selectExpr("age AS col1", "name AS col2").write.insertInto("tblA")
 |      >>> spark.read.table("tblA").sort("age").show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      |100|Hyukjin Kwon|
 |      |120|Hyukjin Kwon|
 |      |120|Hyukjin Kwon|
 |      |140| Haejoon Lee|
 |      |140| Haejoon Lee|
 |      +---+------------+
 |      >>> _ = spark.sql("DROP TABLE tblA")
 |  
 |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None
 |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      table : str
 |          Name of the table in the external database.
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.
 |      properties : dict
 |          a dictionary of JDBC database connection arguments. Normally at
 |          least properties "user" and "password" with their corresponding values.
 |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Notes
 |      -----
 |      Don't create too many partitions in parallel on a large cluster;
 |      otherwise Spark might crash your external database systems.
 |  
 |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None
 |      Saves the content of the :class:`DataFrame` in JSON format
 |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the
 |      specified path.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str
 |          the path in any Hadoop supported file system
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a JSON file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a JSON file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.json(d, mode="overwrite")
 |      ...
 |      ...     # Read the JSON file as a DataFrame.
 |      ...     spark.read.format("json").load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'
 |      Specifies the behavior when data or table already exists.
 |      
 |      Options include:
 |      
 |      * `append`: Append contents of this :class:`DataFrame` to existing data.
 |      * `overwrite`: Overwrite existing data.
 |      * `error` or `errorifexists`: Throw an exception if data already exists.
 |      * `ignore`: Silently ignore this operation if data already exists.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Examples
 |      --------
 |      Raise an error when writing to an existing path.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 80, "name": "Xinrong Meng"}]
 |      ...     ).write.mode("error").format("parquet").save(d) # doctest: +SKIP
 |      Traceback (most recent call last):
 |          ...
 |      ...AnalysisException: ...
 |      
 |      Write a Parquet file back with various options, and read it back.
 |      
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Overwrite the path with a new Parquet file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.mode("overwrite").format("parquet").save(d)
 |      ...
 |      ...     # Append another DataFrame into the Parquet file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 120, "name": "Takuya Ueshin"}]
 |      ...     ).write.mode("append").format("parquet").save(d)
 |      ...
 |      ...     # Append another DataFrame into the Parquet file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 140, "name": "Haejoon Lee"}]
 |      ...     ).write.mode("ignore").format("parquet").save(d)
 |      ...
 |      ...     # Read the Parquet file as a DataFrame.
 |      ...     spark.read.parquet(d).show()
 |      +---+-------------+
 |      |age|         name|
 |      +---+-------------+
 |      |120|Takuya Ueshin|
 |      |100| Hyukjin Kwon|
 |      +---+-------------+
 |  
 |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'
 |      Adds an output option for the underlying data source.
 |      
 |      .. versionadded:: 1.5.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      key : str
 |          The key for the option to set.
 |      value
 |          The value for the option to set.
 |      
 |      Examples
 |      --------
 |      >>> spark.range(1).write.option("key", "value")
 |      <...readwriter.DataFrameWriter object ...>
 |      
 |      Specify the option 'nullValue' with writing a CSV file.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.
 |      ...     df = spark.createDataFrame([(100, None)], "age INT, name STRING")
 |      ...     df.write.option("nullValue", "Hyukjin Kwon").mode("overwrite").format("csv").save(d)
 |      ...
 |      ...     # Read the CSV file as a DataFrame.
 |      ...     spark.read.schema(df.schema).format('csv').load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'
 |      Adds output options for the underlying data source.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      **options : dict
 |          The dictionary of string keys and primitive-type values.
 |      
 |      Examples
 |      --------
 |      >>> spark.range(1).write.option("key", "value")
 |      <...readwriter.DataFrameWriter object ...>
 |      
 |      Specify the option 'nullValue' and 'header' with writing a CSV file.
 |      
 |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType
 |      >>> schema = StructType([
 |      ...     StructField("age",IntegerType(),True),
 |      ...     StructField("name",StringType(),True),
 |      ... ])
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',
 |      ...     # and 'header' option set to `True`.
 |      ...     df = spark.createDataFrame([(100, None)], schema=schema)
 |      ...     df.write.options(nullValue="Hyukjin Kwon", header=True).mode(
 |      ...         "overwrite").format("csv").save(d)
 |      ...
 |      ...     # Read the CSV file as a DataFrame.
 |      ...     spark.read.option("header", True).format('csv').load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None
 |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.
 |      
 |      .. versionadded:: 1.5.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str
 |          the path in any Hadoop supported file system
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.
 |      partitionBy : str or list, optional
 |          names of partitioning columns
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a ORC file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a ORC file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.orc(d, mode="overwrite")
 |      ...
 |      ...     # Read the Parquet file as a DataFrame.
 |      ...     spark.read.format("orc").load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None
 |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str
 |          the path in any Hadoop supported file system
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.
 |      partitionBy : str or list, optional
 |          names of partitioning columns
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a Parquet file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a Parquet file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.parquet(d, mode="overwrite")
 |      ...
 |      ...     # Read the Parquet file as a DataFrame.
 |      ...     spark.read.format("parquet").load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'
 |      Partitions the output by the given columns on the file system.
 |      
 |      If specified, the output is laid out on the file system similar
 |      to Hive's partitioning scheme.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      cols : str or list
 |          name of columns
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.
 |      
 |      >>> import tempfile
 |      >>> import os
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}, {"age": 120, "name": "Ruifeng Zheng"}]
 |      ...     ).write.partitionBy("name").mode("overwrite").format("parquet").save(d)
 |      ...
 |      ...     # Read the Parquet file as a DataFrame.
 |      ...     spark.read.parquet(d).sort("age").show()
 |      ...
 |      ...     # Read one partition as a DataFrame.
 |      ...     spark.read.parquet(f"{d}{os.path.sep}name=Hyukjin Kwon").show()
 |      +---+-------------+
 |      |age|         name|
 |      +---+-------------+
 |      |100| Hyukjin Kwon|
 |      |120|Ruifeng Zheng|
 |      +---+-------------+
 |      +---+
 |      |age|
 |      +---+
 |      |100|
 |      +---+
 |  
 |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None
 |      Saves the contents of the :class:`DataFrame` to a data source.
 |      
 |      The data source is specified by the ``format`` and a set of ``options``.
 |      If ``format`` is not specified, the default data source configured by
 |      ``spark.sql.sources.default`` will be used.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str, optional
 |          the path in a Hadoop supported file system
 |      format : str, optional
 |          the format used to save
 |      mode : str, optional
 |          specifies the behavior of the save operation when data already exists.
 |      
 |          * ``append``: Append contents of this :class:`DataFrame` to existing data.
 |          * ``overwrite``: Overwrite existing data.
 |          * ``ignore``: Silently ignore this operation if data already exists.
 |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.
 |      partitionBy : list, optional
 |          names of partitioning columns
 |      **options : dict
 |          all other string options
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a JSON file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a JSON file
 |      ...     spark.createDataFrame(
 |      ...         [{"age": 100, "name": "Hyukjin Kwon"}]
 |      ...     ).write.mode("overwrite").format("json").save(d)
 |      ...
 |      ...     # Read the JSON file as a DataFrame.
 |      ...     spark.read.format('json').load(d).show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      +---+------------+
 |  
 |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None
 |      Saves the content of the :class:`DataFrame` as the specified table.
 |      
 |      In the case the table already exists, behavior of this function depends on the
 |      save mode, specified by the `mode` function (default to throwing an exception).
 |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be
 |      the same as that of the existing table.
 |      
 |      * `append`: Append contents of this :class:`DataFrame` to existing data.
 |      * `overwrite`: Overwrite existing data.
 |      * `error` or `errorifexists`: Throw an exception if data already exists.
 |      * `ignore`: Silently ignore this operation if data already exists.
 |      
 |      .. versionadded:: 1.4.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Notes
 |      -----
 |      When `mode` is `Append`, if there is an existing table, we will use the format and
 |      options of the existing table. The column order in the schema of the :class:`DataFrame`
 |      doesn't need to be the same as that of the existing table. Unlike
 |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the
 |      column names to find the correct column positions.
 |      
 |      Parameters
 |      ----------
 |      name : str
 |          the table name
 |      format : str, optional
 |          the format used to save
 |      mode : str, optional
 |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)
 |      partitionBy : str or list
 |          names of partitioning columns
 |      **options : dict
 |          all other string options
 |      
 |      Examples
 |      --------
 |      Creates a table from a DataFrame, and read it back.
 |      
 |      >>> _ = spark.sql("DROP TABLE IF EXISTS tblA")
 |      >>> spark.createDataFrame([
 |      ...     (100, "Hyukjin Kwon"), (120, "Hyukjin Kwon"), (140, "Haejoon Lee")],
 |      ...     schema=["age", "name"]
 |      ... ).write.saveAsTable("tblA")
 |      >>> spark.read.table("tblA").sort("age").show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      |120|Hyukjin Kwon|
 |      |140| Haejoon Lee|
 |      +---+------------+
 |      >>> _ = spark.sql("DROP TABLE tblA")
 |  
 |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'
 |      Sorts the output in each bucket by the given columns on the file system.
 |      
 |      .. versionadded:: 2.3.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      col : str, tuple or list
 |          a name of a column, or a list of names.
 |      cols : str
 |          additional names (optional). If `col` is a list it should be empty.
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.
 |      
 |      >>> from pyspark.sql.functions import input_file_name
 |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.
 |      ... _ = spark.sql("DROP TABLE IF EXISTS sorted_bucketed_table")
 |      >>> spark.createDataFrame([
 |      ...     (100, "Hyukjin Kwon"), (120, "Hyukjin Kwon"), (140, "Haejoon Lee")],
 |      ...     schema=["age", "name"]
 |      ... ).write.bucketBy(1, "name").sortBy("age").mode(
 |      ...     "overwrite").saveAsTable("sorted_bucketed_table")
 |      >>> # Read the Parquet file as a DataFrame.
 |      ... spark.read.table("sorted_bucketed_table").sort("age").show()
 |      +---+------------+
 |      |age|        name|
 |      +---+------------+
 |      |100|Hyukjin Kwon|
 |      |120|Hyukjin Kwon|
 |      |140| Haejoon Lee|
 |      +---+------------+
 |      >>> _ = spark.sql("DROP TABLE sorted_bucketed_table")
 |  
 |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None
 |      Saves the content of the DataFrame in a text file at the specified path.
 |      The text files will be encoded as UTF-8.
 |      
 |      .. versionadded:: 1.6.0
 |      
 |      .. versionchanged:: 3.4.0
 |          Supports Spark Connect.
 |      
 |      Parameters
 |      ----------
 |      path : str
 |          the path in any Hadoop supported file system
 |      
 |      Other Parameters
 |      ----------------
 |      Extra options
 |          For the extra options, refer to
 |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_
 |          for the version you use.
 |      
 |          .. # noqa
 |      
 |      Notes
 |      -----
 |      The DataFrame must have only one column that is of string type.
 |      Each row becomes a new line in the output file.
 |      
 |      Examples
 |      --------
 |      Write a DataFrame into a text file and read it back.
 |      
 |      >>> import tempfile
 |      >>> with tempfile.TemporaryDirectory() as d:
 |      ...     # Write a DataFrame into a text file
 |      ...     df = spark.createDataFrame([("a",), ("b",), ("c",)], schema=["alphabets"])
 |      ...     df.write.mode("overwrite").text(d)
 |      ...
 |      ...     # Read the text file as a DataFrame.
 |      ...     spark.read.schema(df.schema).format("text").load(d).sort("alphabets").show()
 |      +---------+
 |      |alphabets|
 |      +---------+
 |      |        a|
 |      |        b|
 |      |        c|
 |      +---------+
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from OptionUtils:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)

