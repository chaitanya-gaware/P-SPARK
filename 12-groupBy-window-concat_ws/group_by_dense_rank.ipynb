{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 21:44:03 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.25 instead (on interface wlp0s20f3)\n",
      "24/04/17 21:44:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/17 21:44:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName(\"group_window\")\\\n",
    "                    .getOrCreate()\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-----------+\n",
      "| ID|Product Name| Price|Total Value|\n",
      "+---+------------+------+-----------+\n",
      "|  1|      Jacket|872.19|    7849.71|\n",
      "|  2|       Jeans|865.39|    1730.78|\n",
      "|  3|         Hat|183.11|     549.33|\n",
      "|  4|     Sweater|305.15|      610.3|\n",
      "|  5|         Hat| 84.59|      845.9|\n",
      "|  6|       Socks|191.72|    1533.76|\n",
      "|  7|      Jacket|696.02|     696.02|\n",
      "|  8|     Sweater|170.31|    1532.79|\n",
      "|  9|       Jeans|163.04|    1141.28|\n",
      "| 10|     Sweater| 990.3|     1980.6|\n",
      "| 11|     Sweater| 92.88|     743.04|\n",
      "| 12|       Shoes|306.33|    1531.65|\n",
      "| 13|       Shirt| 65.67|     197.01|\n",
      "| 14|      Jacket|555.63|    4445.04|\n",
      "| 15|       Jeans| 48.99|     342.93|\n",
      "| 16|      Jacket|323.87|    2590.96|\n",
      "| 17|      Jacket| 895.2|     8952.0|\n",
      "| 18|       Shoes|106.37|     531.85|\n",
      "| 19|         Hat|146.77|     880.62|\n",
      "| 20|       Socks|584.94|    5264.46|\n",
      "+---+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",True)\\\n",
    "                .option(\"inferschema\",True)\\\n",
    "                .load(\"./sales_transactions.csv\")\n",
    "                \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>  (191 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Product Name|       total_value|\n",
      "+------------+------------------+\n",
      "|      Jacket| 378274.5700000001|\n",
      "|       Jeans|362197.52999999985|\n",
      "|       Socks| 299744.8399999999|\n",
      "|       Dress| 299719.0699999999|\n",
      "|       Shirt| 274149.0500000001|\n",
      "|       Pants|         269397.16|\n",
      "|         Hat|251633.72999999995|\n",
      "|       Shoes|         248539.26|\n",
      "|     Sweater|246963.95000000004|\n",
      "|       Skirt|211751.48000000013|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(col('Product Name')).agg(sum(col(\"Total Value\")).alias(\"total_value\")).orderBy(desc(col(\"total_value\")))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_specs = Window.orderBy(col(\"total_value\"),desc())\n",
    "window_specs = Window.orderBy(desc(\"total_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 21:44:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 8:==================================================>    (183 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+----+\n",
      "|Product Name|       total_value|rank|\n",
      "+------------+------------------+----+\n",
      "|       Jeans|362197.52999999985|   2|\n",
      "|       Socks| 299744.8399999999|   3|\n",
      "|       Dress| 299719.0699999999|   4|\n",
      "|       Shirt| 274149.0500000001|   5|\n",
      "+------------+------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_specs = Window.orderBy(desc(\"total_value\"))\n",
    "df2 = df1.withColumn(\"rank\", dense_rank().over(window_specs))\n",
    "dff = df2.filter((col(\"rank\") < 6) & (col(\"rank\") > 1))\n",
    "dff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 21:44:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 14:=================================================>    (183 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----+\n",
      "|Product Name|      total_value|rank|\n",
      "+------------+-----------------+----+\n",
      "|       Socks|299744.8399999999|   3|\n",
      "|       Dress|299719.0699999999|   4|\n",
      "|       Shirt|274149.0500000001|   5|\n",
      "+------------+-----------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df4 = df2.filter((col(\"rank\") < 6) & (col(\"rank\") > 2))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 21:44:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 20:====================================================> (196 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+----+\n",
      "|Product Name|       total_value|rank|\n",
      "+------------+------------------+----+\n",
      "|      Jacket| 378274.5700000001|   1|\n",
      "|       Jeans|362197.52999999985|   2|\n",
      "|       Socks| 299744.8399999999|   3|\n",
      "|       Dress| 299719.0699999999|   4|\n",
      "|       Shirt| 274149.0500000001|   5|\n",
      "|       Pants|         269397.16|   6|\n",
      "|         Hat|251633.72999999995|   7|\n",
      "|       Shoes|         248539.26|   8|\n",
      "|     Sweater|246963.95000000004|   9|\n",
      "|       Skirt|211751.48000000013|  10|\n",
      "+------------+------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window_specs = Window.orderBy(desc(\"total_value\"))\n",
    "df2 = df1.withColumn(\"rank\", dense_rank().over(window_specs))\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-----------+\n",
      "| ID|Product Name| Price|Total Value|\n",
      "+---+------------+------+-----------+\n",
      "|  1|      Jacket|872.19|    7849.71|\n",
      "|  2|       Jeans|865.39|    1730.78|\n",
      "|  3|         Hat|183.11|     549.33|\n",
      "|  4|     Sweater|305.15|      610.3|\n",
      "|  5|         Hat| 84.59|      845.9|\n",
      "|  6|       Socks|191.72|    1533.76|\n",
      "|  7|      Jacket|696.02|     696.02|\n",
      "|  8|     Sweater|170.31|    1532.79|\n",
      "|  9|       Jeans|163.04|    1141.28|\n",
      "| 10|     Sweater| 990.3|     1980.6|\n",
      "| 11|     Sweater| 92.88|     743.04|\n",
      "| 12|       Shoes|306.33|    1531.65|\n",
      "| 13|       Shirt| 65.67|     197.01|\n",
      "| 14|      Jacket|555.63|    4445.04|\n",
      "| 15|       Jeans| 48.99|     342.93|\n",
      "| 16|      Jacket|323.87|    2590.96|\n",
      "| 17|      Jacket| 895.2|     8952.0|\n",
      "| 18|       Shoes|106.37|     531.85|\n",
      "| 19|         Hat|146.77|     880.62|\n",
      "| 20|       Socks|584.94|    5264.46|\n",
      "+---+------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+-----------+--------------+\n",
      "| ID|Product Name| Price|Total Value|          sale|\n",
      "+---+------------+------+-----------+--------------+\n",
      "|  1|      Jacket|872.19|    7849.71|872.19|7849.71|\n",
      "|  2|       Jeans|865.39|    1730.78|865.39|1730.78|\n",
      "|  3|         Hat|183.11|     549.33| 183.11|549.33|\n",
      "|  4|     Sweater|305.15|      610.3|  305.15|610.3|\n",
      "|  5|         Hat| 84.59|      845.9|   84.59|845.9|\n",
      "|  6|       Socks|191.72|    1533.76|191.72|1533.76|\n",
      "|  7|      Jacket|696.02|     696.02| 696.02|696.02|\n",
      "|  8|     Sweater|170.31|    1532.79|170.31|1532.79|\n",
      "|  9|       Jeans|163.04|    1141.28|163.04|1141.28|\n",
      "| 10|     Sweater| 990.3|     1980.6|  990.3|1980.6|\n",
      "| 11|     Sweater| 92.88|     743.04|  92.88|743.04|\n",
      "| 12|       Shoes|306.33|    1531.65|306.33|1531.65|\n",
      "| 13|       Shirt| 65.67|     197.01|  65.67|197.01|\n",
      "| 14|      Jacket|555.63|    4445.04|555.63|4445.04|\n",
      "| 15|       Jeans| 48.99|     342.93|  48.99|342.93|\n",
      "| 16|      Jacket|323.87|    2590.96|323.87|2590.96|\n",
      "| 17|      Jacket| 895.2|     8952.0|  895.2|8952.0|\n",
      "| 18|       Shoes|106.37|     531.85| 106.37|531.85|\n",
      "| 19|         Hat|146.77|     880.62| 146.77|880.62|\n",
      "| 20|       Socks|584.94|    5264.46|584.94|5264.46|\n",
      "+---+------------+------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn(\"sale\", concat_ws(\"|\",col(\"Price\"),col(\"Total Value\")))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+\n",
      "| ID|Product Name|    col|\n",
      "+---+------------+-------+\n",
      "|  1|      Jacket| 872.19|\n",
      "|  1|      Jacket|7849.71|\n",
      "|  2|       Jeans| 865.39|\n",
      "|  2|       Jeans|1730.78|\n",
      "|  3|         Hat| 183.11|\n",
      "|  3|         Hat| 549.33|\n",
      "|  4|     Sweater| 305.15|\n",
      "|  4|     Sweater|  610.3|\n",
      "|  5|         Hat|  84.59|\n",
      "|  5|         Hat|  845.9|\n",
      "|  6|       Socks| 191.72|\n",
      "|  6|       Socks|1533.76|\n",
      "|  7|      Jacket| 696.02|\n",
      "|  7|      Jacket| 696.02|\n",
      "|  8|     Sweater| 170.31|\n",
      "|  8|     Sweater|1532.79|\n",
      "|  9|       Jeans| 163.04|\n",
      "|  9|       Jeans|1141.28|\n",
      "| 10|     Sweater|  990.3|\n",
      "| 10|     Sweater| 1980.6|\n",
      "+---+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.select(\"ID\",\"Product Name\", explode(split(col(\"sale\"), \"\\|\")))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`ID`' given input columns: [Product Name];\n'Aggregate ['ID], ['ID, collect_list('col, 0, 0) AS sale#2772]\n+- Project [Product Name#17]\n   +- Project [ID#16, Product Name#17, col#226]\n      +- Generate explode(split(sale#165, \\|, -1)), false, [col#226]\n         +- Project [ID#16, Product Name#17, Price#18, Total Value#19, concat_ws(|, cast(Price#18 as string), cast(Total Value#19 as string)) AS sale#165]\n            +- Relation[ID#16,Product Name#17,Price#18,Total Value#19] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProduct Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/pyspark/sql/group.py:118\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 118\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/GIT/.spark-env/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`ID`' given input columns: [Product Name];\n'Aggregate ['ID], ['ID, collect_list('col, 0, 0) AS sale#2772]\n+- Project [Product Name#17]\n   +- Project [ID#16, Product Name#17, col#226]\n      +- Generate explode(split(sale#165, \\|, -1)), false, [col#226]\n         +- Project [ID#16, Product Name#17, Price#18, Total Value#19, concat_ws(|, cast(Price#18 as string), cast(Total Value#19 as string)) AS sale#165]\n            +- Relation[ID#16,Product Name#17,Price#18,Total Value#19] csv\n"
     ]
    }
   ],
   "source": [
    "df4.select(\"Product Name\").groupBy(\"ID\").agg(collect_list(col(\"col\")).alias(\"sale\")).orderBy(\"ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:=================================================>    (185 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| ID|              sale|\n",
      "+---+------------------+\n",
      "|  1|  [Jacket, Jacket]|\n",
      "|  2|    [Jeans, Jeans]|\n",
      "|  3|        [Hat, Hat]|\n",
      "|  4|[Sweater, Sweater]|\n",
      "|  5|        [Hat, Hat]|\n",
      "|  6|    [Socks, Socks]|\n",
      "|  7|  [Jacket, Jacket]|\n",
      "|  8|[Sweater, Sweater]|\n",
      "|  9|    [Jeans, Jeans]|\n",
      "| 10|[Sweater, Sweater]|\n",
      "| 11|[Sweater, Sweater]|\n",
      "| 12|    [Shoes, Shoes]|\n",
      "| 13|    [Shirt, Shirt]|\n",
      "| 14|  [Jacket, Jacket]|\n",
      "| 15|    [Jeans, Jeans]|\n",
      "| 16|  [Jacket, Jacket]|\n",
      "| 17|  [Jacket, Jacket]|\n",
      "| 18|    [Shoes, Shoes]|\n",
      "| 19|        [Hat, Hat]|\n",
      "| 20|    [Socks, Socks]|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "df4.groupBy(\"ID\").agg(collect_list(\"Product Name\").alias(\"sale\")).orderBy(\"ID\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
