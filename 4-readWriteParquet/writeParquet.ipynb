{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 00:55:01 WARN Utils: Your hostname, chaitanya resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp0s20f3)\n",
      "23/11/18 00:55:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/18 00:55:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/18 00:55:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                .appName(\"ReadParquet\")\\\n",
    "                .master(\"local\") \\\n",
    "                .getOrCreate()\n",
    "                \n",
    "schema = None\n",
    "opt = {\"inferschema\":True, \"mode\":\"permissive\", \"header\":True, \"escape\":\"\\\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 00:55:13 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "            .options(**opt)\\\n",
    "            .load(\"currency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+-----+\n",
      "|Code|Symbol|                Name|value|\n",
      "+----+------+--------------------+-----+\n",
      "| AED|   د.إ|United Arab Emira...|   10|\n",
      "| AFN|     ؋|      Afghan afghani|   20|\n",
      "| ALL|     L|       Albanian, lek|   30|\n",
      "| AMD|   AMD|       Armenian dram|   40|\n",
      "| ANG|     ƒ|Netherlands Antil...|   50|\n",
      "| AOA|    Kz|      Angolan kwanza| NULL|\n",
      "| ARS|     $|      Argentine peso| NULL|\n",
      "| AUD|     $|   Australian dollar| NULL|\n",
      "| AWG|  Afl.|       Aruban florin| NULL|\n",
      "| AZN|   AZN|   Azerbaijani manat| NULL|\n",
      "| BAM|    KM|Bosnia and Herzeg...| NULL|\n",
      "| BBD|     $|    Barbadian dollar| NULL|\n",
      "| BDT|    ৳ |    Bangladeshi taka| NULL|\n",
      "| BGN|   лв.|       Bulgarian lev| NULL|\n",
      "| BHD|  .د.ب|      Bahraini dinar| NULL|\n",
      "| BIF|    Fr|     Burundian franc| NULL|\n",
      "| BMD|     $|    Bermudian dollar| NULL|\n",
      "| BND|     $|       Brunei dollar| NULL|\n",
      "| BOB|   Bs.|  Bolivian boliviano| NULL|\n",
      "| BRL|    R$|      Brazilian real| NULL|\n",
      "+----+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Symbol: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/home/gson/GIT/P-SPARK/4-readWriteParquet/save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parquet in module pyspark.sql.readwriter:\n",
      "\n",
      "parquet(path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      "    \n",
      "    .. versionadded:: 1.4.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str\n",
      "        the path in any Hadoop supported file system\n",
      "    mode : str, optional\n",
      "        specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    partitionBy : str or list, optional\n",
      "        names of partitioning columns\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      "        for the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Write a DataFrame into a Parquet file and read it back.\n",
      "    \n",
      "    >>> import tempfile\n",
      "    >>> with tempfile.TemporaryDirectory() as d:\n",
      "    ...     # Write a DataFrame into a Parquet file\n",
      "    ...     spark.createDataFrame(\n",
      "    ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      "    ...     ).write.parquet(d, mode=\"overwrite\")\n",
      "    ...\n",
      "    ...     # Read the Parquet file as a DataFrame.\n",
      "    ...     spark.read.format(\"parquet\").load(d).show()\n",
      "    +---+------------+\n",
      "    |age|        name|\n",
      "    +---+------------+\n",
      "    |100|Hyukjin Kwon|\n",
      "    +---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.write.parquet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
