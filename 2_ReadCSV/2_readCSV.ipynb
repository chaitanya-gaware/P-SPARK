{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 23:58:31 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp0s20f3)\n",
      "23/12/09 23:58:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/09 23:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"ReadCSV\")\\\n",
    "                    .master(\"local\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_field = [StructField(\"code\", StringType(), True),\n",
    "                 StructField(\"symbol\", StringType(), True),\n",
    "                 StructField(\"Name\", StringType(), True),\n",
    "                 StructField(\"value\",IntegerType(), True),\n",
    "                 StructField(\"_corrupt_record\", StringType(), True)]\n",
    "schema = StructType(schema_field)\n",
    "opt = {\"header\":True, \"inferSchema\":False, \"mode\":\"permissive\",\"escape\":\"\\\"\", \"nullValue\":20}\n",
    "#here escape is used to escape the comma < , > in the double qoute <\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "                .options(**opt)\\\n",
    "                .schema(schema)\\\n",
    "                .load(\"currency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+-----+--------------------+\n",
      "|code|symbol|                Name|value|     _corrupt_record|\n",
      "+----+------+--------------------+-----+--------------------+\n",
      "| AED|   د.إ|United Arab Emira...|   10|                NULL|\n",
      "| AFN|     ؋|      Afghan afghani| NULL|AFN,؋,Afghan afgh...|\n",
      "| ALL|     L|       Albanian, lek|   30|                NULL|\n",
      "| AMD|   AMD|       Armenian dram|   40|                NULL|\n",
      "| ANG|     ƒ|Netherlands Antil...|   50|                NULL|\n",
      "| AOA|    Kz|      Angolan kwanza| NULL|                NULL|\n",
      "| ARS|     $|      Argentine peso| NULL|                NULL|\n",
      "| AUD|     $|   Australian dollar| NULL|                NULL|\n",
      "| AWG|  Afl.|       Aruban florin| NULL|                NULL|\n",
      "| AZN|   AZN|   Azerbaijani manat| NULL|                NULL|\n",
      "| BAM|    KM|Bosnia and Herzeg...| NULL|                NULL|\n",
      "| BBD|     $|    Barbadian dollar| NULL|                NULL|\n",
      "| BDT|    ৳ |    Bangladeshi taka| NULL|                NULL|\n",
      "| BGN|   лв.|       Bulgarian lev| NULL|                NULL|\n",
      "| BHD|  .د.ب|      Bahraini dinar| NULL|                NULL|\n",
      "| BIF|    Fr|     Burundian franc| NULL|                NULL|\n",
      "| BMD|     $|    Bermudian dollar| NULL|                NULL|\n",
      "| BND|     $|       Brunei dollar| NULL|                NULL|\n",
      "| BOB|   Bs.|  Bolivian boliviano| NULL|                NULL|\n",
      "| BRL|    R$|      Brazilian real| NULL|                NULL|\n",
      "+----+------+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------+-----+--------------------+\n",
      "|code|symbol|          Name|value|     _corrupt_record|\n",
      "+----+------+--------------+-----+--------------------+\n",
      "| AFN|     ؋|Afghan afghani| NULL|AFN,؋,Afghan afgh...|\n",
      "+----+------+--------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"_corrupt_record\"] != 'NULL').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark)\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None)\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |          string, or list of strings, for input path(s),\n",
      " |          or RDD of Strings storing CSV rows.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      sep : str, optional\n",
      " |          sets a separator (one or more characters) for each field and value. If None is\n",
      " |          set, it uses the default value, ``,``.\n",
      " |      encoding : str, optional\n",
      " |          decodes the CSV files by the given encoding type. If None is set,\n",
      " |          it uses the default value, ``UTF-8``.\n",
      " |      quote : str, optional\n",
      " |          sets a single character used for escaping quoted values where the\n",
      " |          separator can be part of the value. If None is set, it uses the default\n",
      " |          value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      " |          empty string.\n",
      " |      escape : str, optional\n",
      " |          sets a single character used for escaping quotes inside an already\n",
      " |          quoted value. If None is set, it uses the default value, ``\\``.\n",
      " |      comment : str, optional\n",
      " |          sets a single character used for skipping lines beginning with this\n",
      " |          character. By default (None), it is disabled.\n",
      " |      header : str or bool, optional\n",
      " |          uses the first line as names of columns. If None is set, it uses the\n",
      " |          default value, ``false``.\n",
      " |      \n",
      " |          .. note:: if the given path is a RDD of Strings, this header\n",
      " |              option will remove all lines same with the header if exists.\n",
      " |      \n",
      " |      inferSchema : str or bool, optional\n",
      " |          infers the input schema automatically from data. It requires one extra\n",
      " |          pass over the data. If None is set, it uses the default value, ``false``.\n",
      " |      enforceSchema : str or bool, optional\n",
      " |          If it is set to ``true``, the specified or inferred schema will be\n",
      " |          forcibly applied to datasource files, and headers in CSV files will be\n",
      " |          ignored. If the option is set to ``false``, the schema will be\n",
      " |          validated against all headers in CSV files or the first header in RDD\n",
      " |          if the ``header`` option is set to ``true``. Field names in the schema\n",
      " |          and column names in CSV headers are checked by their positions\n",
      " |          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      " |          ``true`` is used by default. Though the default value is ``true``,\n",
      " |          it is recommended to disable the ``enforceSchema`` option\n",
      " |          to avoid incorrect results.\n",
      " |      ignoreLeadingWhiteSpace : str or bool, optional\n",
      " |          A flag indicating whether or not leading whitespaces from\n",
      " |          values being read should be skipped. If None is set, it\n",
      " |          uses the default value, ``false``.\n",
      " |      ignoreTrailingWhiteSpace : str or bool, optional\n",
      " |          A flag indicating whether or not trailing whitespaces from\n",
      " |          values being read should be skipped. If None is set, it\n",
      " |          uses the default value, ``false``.\n",
      " |      nullValue : str, optional\n",
      " |          sets the string representation of a null value. If None is set, it uses\n",
      " |          the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      " |          applies to all supported types including the string type.\n",
      " |      nanValue : str, optional\n",
      " |          sets the string representation of a non-number value. If None is set, it\n",
      " |          uses the default value, ``NaN``.\n",
      " |      positiveInf : str, optional\n",
      " |          sets the string representation of a positive infinity value. If None\n",
      " |          is set, it uses the default value, ``Inf``.\n",
      " |      negativeInf : str, optional\n",
      " |          sets the string representation of a negative infinity value. If None\n",
      " |          is set, it uses the default value, ``Inf``.\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats\n",
      " |          follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      maxColumns : str or int, optional\n",
      " |          defines a hard limit of how many columns a record can have. If None is\n",
      " |          set, it uses the default value, ``20480``.\n",
      " |      maxCharsPerColumn : str or int, optional\n",
      " |          defines the maximum number of characters allowed for any given\n",
      " |          value being read. If None is set, it uses the default value,\n",
      " |          ``-1`` meaning unlimited length.\n",
      " |      maxMalformedLogPerPartition : str or int, optional\n",
      " |          this parameter is no longer used since Spark 2.2.0.\n",
      " |          If specified, it is ignored.\n",
      " |      mode : str, optional\n",
      " |          allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |          set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      " |          parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      " |          records can be different based on required set of fields. This behavior can\n",
      " |          be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      " |          (enabled by default).\n",
      " |      \n",
      " |          * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string \\\n",
      " |            into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\n",
      " |            fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      " |            field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      " |            schema does not have the field, it drops corrupt records during parsing. \\\n",
      " |            A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      " |            When it meets a record having fewer tokens than the length of the schema, \\\n",
      " |            sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      " |            length of the schema, it drops extra tokens.\n",
      " |          * ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      " |          * ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      columnNameOfCorruptRecord : str, optional\n",
      " |          allows renaming the new field having malformed string\n",
      " |          created by ``PERMISSIVE`` mode. This overrides\n",
      " |          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |          it uses the value specified in\n",
      " |          ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      multiLine : str or bool, optional\n",
      " |          parse records, which may span multiple lines. If None is\n",
      " |          set, it uses the default value, ``false``.\n",
      " |      charToEscapeQuoteEscaping : str, optional\n",
      " |          sets a single character used for escaping the escape for\n",
      " |          the quote character. If None is set, the default value is\n",
      " |          escape character when escape and quote characters are\n",
      " |          different, ``\\0`` otherwise.\n",
      " |      samplingRatio : str or float, optional\n",
      " |          defines fraction of rows used for schema inferring.\n",
      " |          If None is set, it uses the default value, ``1.0``.\n",
      " |      emptyValue : str, optional\n",
      " |          sets the string representation of an empty value. If None is set, it uses\n",
      " |          the default value, empty string.\n",
      " |      locale : str, optional\n",
      " |          sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      " |          it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      " |          parsing dates and timestamps.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for parsing. If None is\n",
      " |          set, it covers all ``\\\\r``, ``\\\\r\\\\n`` and ``\\\\n``.\n",
      " |          Maximum length is 1 character.\n",
      " |      pathGlobFilter : str or bool, optional\n",
      " |          an optional glob pattern to only include files with paths matching\n",
      " |          the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      " |          It does not change the behavior of\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      recursiveFileLookup : str or bool, optional\n",
      " |          recursively scan a directory for files. Using this option disables\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      \n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedBefore (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedAfter (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring after the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      unescapedQuoteHandling : str, optional\n",
      " |          defines how the CsvParser will handle values with unescaped quotes. If None is\n",
      " |          set, it uses the default value, ``STOP_AT_DELIMITER``.\n",
      " |      \n",
      " |          * ``STOP_AT_CLOSING_QUOTE``: If unescaped quotes are found in the input, accumulate\n",
      " |            the quote character and proceed parsing the value as a quoted value, until a closing\n",
      " |            quote is found.\n",
      " |          * ``BACK_TO_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      " |            as an unquoted value. This will make the parser accumulate all characters of the current\n",
      " |            parsed value until the delimiter is found. If no delimiter is found in the value, the\n",
      " |            parser will continue accumulating characters from the input until a delimiter or line\n",
      " |            ending is found.\n",
      " |          * ``STOP_AT_DELIMITER``: If unescaped quotes are found in the input, consider the value\n",
      " |            as an unquoted value. This will make the parser accumulate all characters until the\n",
      " |            delimiter or a line ending is found in the input.\n",
      " |          * ``SKIP_VALUE``: If unescaped quotes are found in the input, the content parsed\n",
      " |            for the given value will be skipped and the value set in nullValue will be produced\n",
      " |            instead.\n",
      " |          * ``RAISE_ERROR``: If unescaped quotes are found in the input, a TextParsingException\n",
      " |            will be thrown.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      " |      >>> df.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      " |      >>> df2 = spark.read.csv(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound``, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      url : str\n",
      " |          a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      table : str\n",
      " |          the name of the table\n",
      " |      column : str, optional\n",
      " |          the name of a column of numeric, date, or timestamp type\n",
      " |          that will be used for partitioning;\n",
      " |          if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
      " |          (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
      " |          for generated WHERE clause expressions used to split the column\n",
      " |          ``column`` evenly\n",
      " |      lowerBound : str or int, optional\n",
      " |          the minimum value of ``column`` used to decide partition stride\n",
      " |      upperBound : str or int, optional\n",
      " |          the maximum value of ``column`` used to decide partition stride\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions\n",
      " |      predicates : list, optional\n",
      " |          a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |          each one defines one partition of the :class:`DataFrame`\n",
      " |      properties : dict, optional\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |  \n",
      " |  json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None, locale=None, pathGlobFilter=None, recursiveFileLookup=None, allowNonNumericNumbers=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, list or :class:`RDD`\n",
      " |          string represents path to the JSON dataset, or a list of paths,\n",
      " |          or RDD of Strings storing JSON objects.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |          a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      primitivesAsString : str or bool, optional\n",
      " |          infers all primitive values as a string type. If None is set,\n",
      " |          it uses the default value, ``false``.\n",
      " |      prefersDecimal : str or bool, optional\n",
      " |          infers all floating-point values as a decimal type. If the values\n",
      " |          do not fit in decimal, then it infers them as doubles. If None is\n",
      " |          set, it uses the default value, ``false``.\n",
      " |      allowComments : str or bool, optional\n",
      " |          ignores Java/C++ style comment in JSON records. If None is set,\n",
      " |          it uses the default value, ``false``.\n",
      " |      allowUnquotedFieldNames : str or bool, optional\n",
      " |          allows unquoted JSON field names. If None is set,\n",
      " |          it uses the default value, ``false``.\n",
      " |      allowSingleQuotes : str or bool, optional\n",
      " |          allows single quotes in addition to double quotes. If None is\n",
      " |          set, it uses the default value, ``true``.\n",
      " |      allowNumericLeadingZero : str or bool, optional\n",
      " |          allows leading zeros in numbers (e.g. 00012). If None is\n",
      " |          set, it uses the default value, ``false``.\n",
      " |      allowBackslashEscapingAnyCharacter : str or bool, optional\n",
      " |          allows accepting quoting of all character\n",
      " |          using backslash quoting mechanism. If None is\n",
      " |          set, it uses the default value, ``false``.\n",
      " |      mode : str, optional\n",
      " |          allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |                   set, it uses the default value, ``PERMISSIVE``.\n",
      " |      \n",
      " |          * ``PERMISSIVE``: when it meets a corrupted record, puts the malformed string               into a field configured by ``columnNameOfCorruptRecord``, and sets malformed               fields to ``null``. To keep corrupt records, an user can set a string type               field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a               schema does not have the field, it drops corrupt records during parsing.               When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``               field in an output schema.\n",
      " |          *  ``DROPMALFORMED``: ignores the whole corrupted records.\n",
      " |          *  ``FAILFAST``: throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      columnNameOfCorruptRecord: str, optional\n",
      " |          allows renaming the new field having malformed string\n",
      " |          created by ``PERMISSIVE`` mode. This overrides\n",
      " |          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |          it uses the value specified in\n",
      " |          ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      dateFormat : str, optional\n",
      " |          sets the string that indicates a date format. Custom date formats\n",
      " |          follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to date type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd``.\n",
      " |      timestampFormat : str, optional\n",
      " |          sets the string that indicates a timestamp format.\n",
      " |          Custom date formats follow the formats at\n",
      " |          `datetime pattern <https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html>`_.  # noqa\n",
      " |          This applies to timestamp type. If None is set, it uses the\n",
      " |          default value, ``yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]``.\n",
      " |      multiLine : str or bool, optional\n",
      " |          parse one record, which may span multiple lines, per file. If None is\n",
      " |          set, it uses the default value, ``false``.\n",
      " |      allowUnquotedControlChars : str or bool, optional\n",
      " |          allows JSON Strings to contain unquoted control\n",
      " |          characters (ASCII characters with value less than 32,\n",
      " |          including tab and line feed characters) or not.\n",
      " |      encoding : str or bool, optional\n",
      " |          allows to forcibly set one of standard basic or extended encoding for\n",
      " |          the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
      " |          the encoding of input JSON will be detected automatically\n",
      " |          when the multiLine option is set to ``true``.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for parsing. If None is\n",
      " |          set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      " |      samplingRatio : str or float, optional\n",
      " |          defines fraction of input JSON objects used for schema inferring.\n",
      " |          If None is set, it uses the default value, ``1.0``.\n",
      " |      dropFieldIfAllNull : str or bool, optional\n",
      " |          whether to ignore column of all null values or empty\n",
      " |          array/struct during schema inference. If None is set, it\n",
      " |          uses the default value, ``false``.\n",
      " |      locale : str, optional\n",
      " |          sets a locale as language tag in IETF BCP 47 format. If None is set,\n",
      " |          it uses the default value, ``en-US``. For instance, ``locale`` is used while\n",
      " |          parsing dates and timestamps.\n",
      " |      pathGlobFilter : str or bool, optional\n",
      " |          an optional glob pattern to only include files with paths matching\n",
      " |          the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      " |          It does not change the behavior of\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      recursiveFileLookup : str or bool, optional\n",
      " |          recursively scan a directory for files. Using this option\n",
      " |          disables\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      allowNonNumericNumbers : str or bool\n",
      " |          allows JSON parser to recognize set of \"Not-a-Number\" (NaN)\n",
      " |          tokens as legal floating number values. If None is set,\n",
      " |          it uses the default value, ``true``.\n",
      " |      \n",
      " |              * ``+INF``: for positive infinity, as well as alias of\n",
      " |                          ``+Infinity`` and ``Infinity``.\n",
      " |              *  ``-INF``: for negative infinity, alias ``-Infinity``.\n",
      " |              *  ``NaN``: for other not-a-numbers, like result of division by zero.\n",
      " |      modifiedBefore : an optional timestamp to only include files with\n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedAfter : an optional timestamp to only include files with\n",
      " |          modification times occurring after the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      " |      >>> df1.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      " |      >>> df2 = spark.read.json(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  load(self, path=None, format=None, schema=None, **options)\n",
      " |      Loads data from a data source and returns it as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list, optional\n",
      " |          optional string or a list of string for file-system backed data sources.\n",
      " |      format : str, optional\n",
      " |          optional string for format of the data source. Default to 'parquet'.\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      " |          optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |          or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
      " |      ...     opt1=True, opt2=1, opt3='str')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      " |      ...     'python/test_support/sql/people1.json'])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to parse\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |          * ``pathGlobFilter``: an optional glob pattern to only include files with paths matching\n",
      " |              the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter.\n",
      " |              It does not change the behavior of partition discovery.\n",
      " |          * ``modifiedBefore``: an optional timestamp to only include files with\n",
      " |              modification times occurring before the specified time. The provided timestamp\n",
      " |              must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |          * ``modifiedAfter``: an optional timestamp to only include files with\n",
      " |              modification times occurring after the specified time. The provided timestamp\n",
      " |              must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a time zone ID to be used to parse\n",
      " |              timestamps in the JSON/CSV datasources or partition values. The following\n",
      " |              formats of `timeZone` are supported:\n",
      " |      \n",
      " |              * Region-based zone ID: It should have the form 'area/city', such as                   'America/Los_Angeles'.\n",
      " |              * Zone offset: It should be in the format '(+|-)HH:mm', for example '-08:00' or                  '+01:00'. Also 'UTC' and 'Z' are supported as aliases of '+00:00'.\n",
      " |      \n",
      " |              Other short names like 'CST' are not recommended to use because they can be\n",
      " |              ambiguous. If it isn't set, the current value of the SQL config\n",
      " |              ``spark.sql.session.timeZone`` is used by default.\n",
      " |          * ``pathGlobFilter``: an optional glob pattern to only include files with paths matching\n",
      " |              the pattern. The syntax follows org.apache.hadoop.fs.GlobFilter.\n",
      " |              It does not change the behavior of partition discovery.\n",
      " |          * ``modifiedBefore``: an optional timestamp to only include files with\n",
      " |              modification times occurring before the specified time. The provided timestamp\n",
      " |              must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |          * ``modifiedAfter``: an optional timestamp to only include files with\n",
      " |              modification times occurring after the specified time. The provided timestamp\n",
      " |              must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path, mergeSchema=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str or list\n",
      " |      mergeSchema : str or bool, optional\n",
      " |          sets whether we should merge schemas collected from all\n",
      " |          ORC part-files. This will override ``spark.sql.orc.mergeSchema``.\n",
      " |          The default value is specified in ``spark.sql.orc.mergeSchema``.\n",
      " |      pathGlobFilter : str or bool\n",
      " |          an optional glob pattern to only include files with paths matching\n",
      " |          the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      " |          It does not change the behavior of\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      recursiveFileLookup : str or bool\n",
      " |          recursively scan a directory for files. Using this option\n",
      " |          disables\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      \n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedBefore : an optional timestamp to only include files with\n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedAfter : an optional timestamp to only include files with\n",
      " |          modification times occurring after the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      " |  \n",
      " |  parquet(self, *paths, **options)\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      mergeSchema : str or bool, optional\n",
      " |          sets whether we should merge schemas collected from all\n",
      " |          Parquet part-files. This will override\n",
      " |          ``spark.sql.parquet.mergeSchema``. The default value is specified in\n",
      " |          ``spark.sql.parquet.mergeSchema``.\n",
      " |      pathGlobFilter : str or bool, optional\n",
      " |          an optional glob pattern to only include files with paths matching\n",
      " |          the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      " |          It does not change the behavior of\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      recursiveFileLookup : str or bool, optional\n",
      " |          recursively scan a directory for files. Using this option\n",
      " |          disables\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      \n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedBefore (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedAfter (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring after the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  schema(self, schema)\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`pyspark.sql.types.StructType` or str\n",
      " |          a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |          (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tableName : str\n",
      " |          string, name of the table.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.createOrReplaceTempView('tmpTable')\n",
      " |      >>> spark.read.table('tmpTable').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |  \n",
      " |  text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None)\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      paths : str or list\n",
      " |          string, or list of strings, for input path(s).\n",
      " |      wholetext : str or bool, optional\n",
      " |          if true, read each file from input path(s) as a single row.\n",
      " |      lineSep : str, optional\n",
      " |          defines the line separator that should be used for parsing. If None is\n",
      " |          set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      " |      pathGlobFilter : str or bool, optional\n",
      " |          an optional glob pattern to only include files with paths matching\n",
      " |          the pattern. The syntax follows `org.apache.hadoop.fs.GlobFilter`.\n",
      " |          It does not change the behavior of\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      recursiveFileLookup : str or bool, optional\n",
      " |          recursively scan a directory for files. Using this option disables\n",
      " |          `partition discovery <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#partition-discovery>`_.  # noqa\n",
      " |      \n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedBefore (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring before the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      modifiedAfter (batch only) : an optional timestamp to only include files with\n",
      " |          modification times occurring after the specified time. The provided timestamp\n",
      " |          must be in the following format: YYYY-MM-DDTHH:mm:ss (e.g. 2020-06-01T13:00:00)\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello'), Row(value='this')]\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello\\nthis')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
