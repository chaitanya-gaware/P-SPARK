{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/03 12:09:36 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp0s20f3)\n",
      "23/12/03 12:09:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/03 12:09:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local[3]\")\\\n",
    "                    .appName(\"case_otherwise\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"header\":True, \"inferSchema\":True, \"escape\":\"\\\"\", \"mode\":\"permissive\"}\n",
    "employee_df = spark.read.format(\"csv\")\\\n",
    "                        .options(**opt)\\\n",
    "                        .load(\"employee_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp ID: integer (nullable = true)\n",
      " |-- Name Prefix: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Middle Initial: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- E_Mail: string (nullable = true)\n",
      " |-- Father's_Name: string (nullable = true)\n",
      " |-- Mother's_Name: string (nullable = true)\n",
      " |-- Mother's_Maiden_Name: string (nullable = true)\n",
      " |-- Date_of_Birth: string (nullable = true)\n",
      " |-- Time_of_Birth: string (nullable = true)\n",
      " |-- Age_in_Yrs: double (nullable = true)\n",
      " |-- Weight_in_Kgs.: integer (nullable = true)\n",
      " |-- Date_of_Joining: string (nullable = true)\n",
      " |-- Quarter_of_Joining: string (nullable = true)\n",
      " |-- Half_of_Joining: string (nullable = true)\n",
      " |-- Year_of_Joining: integer (nullable = true)\n",
      " |-- Month_of_Joining: integer (nullable = true)\n",
      " |-- Month_Name_of_Joining: string (nullable = true)\n",
      " |-- Short_Month: string (nullable = true)\n",
      " |-- Day_of_Joining: integer (nullable = true)\n",
      " |-- DOW_of_Joining: string (nullable = true)\n",
      " |-- Short_DOW: string (nullable = true)\n",
      " |-- Age_in_Company(Years): double (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Last _% _Hike: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- Phone_No: string (nullable = true)\n",
      " |-- Place_Name: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- User Name: string (nullable = true)\n",
      " |-- Password: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# employee_df.show()\n",
    "# employee_df.count()\n",
    "employee_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+----------+\n",
      "|First Name|Last Name|Date_of_Birth|  new_date|\n",
      "+----------+---------+-------------+----------+\n",
      "|  Serafina|Bumgarner|    9/21/1982|1982-09-21|\n",
      "|  Juliette|     Rojo|   05-08-1967|      null|\n",
      "|     Milan| Krawczyk|   04-04-1980|      null|\n",
      "|     Elmer|    Jason|   04-09-1996|      null|\n",
      "|     Zelda|   Forest|   11/27/1959|1959-11-27|\n",
      "|     Rhett|      Wan|    7/14/1976|1976-07-14|\n",
      "|       Hal|   Farrow|    3/15/1967|1967-03-15|\n",
      "|       Del|Fernandez|    8/13/1991|1991-08-13|\n",
      "|     Corey|  Jackman|   04-12-1959|      null|\n",
      "|      Bibi|  Paddock|   10/20/1991|1991-10-20|\n",
      "|      Eric|  Manning|   11-02-1980|      null|\n",
      "|   Renetta|   Hafner|    1/29/1975|1975-01-29|\n",
      "|       Paz|  Pearman|    2/28/1960|1960-02-28|\n",
      "|    Ardath|   Forman|   11-12-1982|      null|\n",
      "|     Nanci|   Osorio|   07-09-1982|      null|\n",
      "|  Maricela|   Simard|    7/21/1988|1988-07-21|\n",
      "|   Avelina|   Stoner|   10-01-1988|      null|\n",
      "| Christene| Mattison|    9/14/1990|1990-09-14|\n",
      "|    Stefan|    Maeda|    3/23/1990|1990-03-23|\n",
      "|   Gillian|   Winter|    1/17/1960|1960-01-17|\n",
      "+----------+---------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "employee_df.select(\"First Name\", \"Last Name\", \"Date_of_Birth\")\\\n",
    "    .withColumn(\n",
    "        \"new_date\", to_date(col(\"Date_of_Birth\"), \"MM/dd/yyyy\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+\n",
      "|First Name|Date_of_Birth|formated_Date|\n",
      "+----------+-------------+-------------+\n",
      "|  Serafina|    9/21/1982|   1982-09-21|\n",
      "|  Juliette|   05-08-1967|   1967-05-08|\n",
      "|     Milan|   04-04-1980|   1980-04-04|\n",
      "|     Elmer|   04-09-1996|   1996-04-09|\n",
      "|     Zelda|   11/27/1959|   1959-11-27|\n",
      "|     Rhett|    7/14/1976|   1976-07-14|\n",
      "|       Hal|    3/15/1967|   1967-03-15|\n",
      "|       Del|    8/13/1991|   1991-08-13|\n",
      "|     Corey|   04-12-1959|   1959-04-12|\n",
      "|      Bibi|   10/20/1991|   1991-10-20|\n",
      "|      Eric|   11-02-1980|   1980-11-02|\n",
      "|   Renetta|    1/29/1975|   1975-01-29|\n",
      "|       Paz|    2/28/1960|   1960-02-28|\n",
      "|    Ardath|   11-12-1982|   1982-11-12|\n",
      "|     Nanci|   07-09-1982|   1982-07-09|\n",
      "|  Maricela|    7/21/1988|   1988-07-21|\n",
      "|   Avelina|   10-01-1988|   1988-10-01|\n",
      "| Christene|    9/14/1990|   1990-09-14|\n",
      "|    Stefan|    3/23/1990|   1990-03-23|\n",
      "|   Gillian|    1/17/1960|   1960-01-17|\n",
      "+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = employee_df.select(\"First Name\",\"Date_of_Birth\").withColumn(\n",
    "    \"formated_Date\",\n",
    "    when(col(\"Date_of_Birth\").contains(\"/\"),\n",
    "         to_date(col(\"Date_of_Birth\"), \"MM/dd/yyyy\")\n",
    "         )\n",
    "    .when(col(\"Date_of_Birth\").contains(\"-\"),\n",
    "          to_date(col(\"Date_of_Birth\"), \"MM-dd-yyyy\")\n",
    "        )\n",
    "    .otherwise(\"None\")\n",
    "        )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 20:14:06 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.13 instead (on interface wlp0s20f3)\n",
      "24/03/28 20:14:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/28 20:14:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|      cst_timestamp|\n",
      "+-------------------+\n",
      "|2024-03-28 10:00:00|\n",
      "|2024-03-29 15:30:00|\n",
      "|2024-03-30 18:45:00|\n",
      "+-------------------+\n",
      "\n",
      "Output DataFrame after converting CST to IST:\n",
      "+-------------------+-------------------+\n",
      "|      cst_timestamp|      ist_timestamp|\n",
      "+-------------------+-------------------+\n",
      "|2024-03-28 10:00:00|2024-03-28 15:30:00|\n",
      "|2024-03-29 15:30:00|2024-03-29 21:00:00|\n",
      "|2024-03-30 18:45:00|2024-03-31 00:15:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CST to IST Conversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample input data with CST timestamps\n",
    "data = [(\"2024-03-28 10:00:00\",),\n",
    "        (\"2024-03-29 15:30:00\",),\n",
    "        (\"2024-03-30 18:45:00\",)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"cst_timestamp\"])\n",
    "\n",
    "# Convert CST to IST\n",
    "result_df = df.withColumn(\"ist_timestamp\", from_utc_timestamp(\"cst_timestamp\", \"IST\"))\n",
    "\n",
    "# Show input DataFrame\n",
    "print(\"Input DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Show output DataFrame\n",
    "print(\"Output DataFrame after converting CST to IST:\")\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
